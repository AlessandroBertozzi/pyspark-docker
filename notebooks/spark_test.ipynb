{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test PySpark Cluster\n",
    "Questo notebook testa le funzionalitÃ  principali del cluster Spark con supporto per Delta Lake, Iceberg e Hudi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inizializzazione Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/02 23:29:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/02 23:29:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session giÃ  attiva\n",
      "Spark Version: 4.0.1\n",
      "App Name: Spark-Notebook\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, max, min\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session giÃ  attiva\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Base DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DataFrame creato\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/02 23:30:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/02 23:30:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/02 23:30:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/12/02 23:30:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# Creazione dati di esempio\n",
    "data = [\n",
    "    (\"Alice\", 25, 85000.0, \"Engineering\"),\n",
    "    (\"Bob\", 30, 75000.0, \"Marketing\"),\n",
    "    (\"Carol\", 35, 95000.0, \"Engineering\"),\n",
    "    (\"David\", 28, 65000.0, \"Sales\"),\n",
    "    (\"Eve\", 32, 88000.0, \"Engineering\"),\n",
    "    (\"Frank\", 27, 72000.0, \"Marketing\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"ðŸ“Š DataFrame creato\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Statistiche per dipartimento:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 14) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------------+-----------------+----------+\n",
      "| department|count|           avg_age|       avg_salary|max_salary|\n",
      "+-----------+-----+------------------+-----------------+----------+\n",
      "|Engineering|    3|30.666666666666668|89333.33333333333|   95000.0|\n",
      "|  Marketing|    2|              28.5|          73500.0|   75000.0|\n",
      "|      Sales|    1|              28.0|          65000.0|   65000.0|\n",
      "+-----------+-----+------------------+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Analisi statistiche\n",
    "print(\"ðŸ“ˆ Statistiche per dipartimento:\")\n",
    "dept_stats = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Performance e Parallelismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset grande creato: 10000 righe\n",
      "âš¡ Test performance aggregazione:\n",
      "+-----------+---------+-----------------+\n",
      "| department|employees|       avg_salary|\n",
      "+-----------+---------+-----------------+\n",
      "|      Sales|     2021|95769.59574468085|\n",
      "|Engineering|     1942| 94423.8455200824|\n",
      "|    Finance|     2056|94232.65758754865|\n",
      "|  Marketing|     2003|94030.96105841239|\n",
      "|         HR|     1978|93188.38877654196|\n",
      "+-----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crea un dataset piÃ¹ grande per testare il parallelismo\n",
    "import random\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"age\", IntegerType(), True),\n",
    "  StructField(\"salary\", DoubleType(), True),  # Usa DoubleType invece di IntegerType\n",
    "  StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "large_data = []\n",
    "departments = [\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"]\n",
    "names = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"]\n",
    "\n",
    "for i in range(10000):\n",
    "    name = f\"{random.choice(names)}_{i}\"\n",
    "    age = random.randint(22, 65)\n",
    "    salary = float(random.randint(40000, 150000))\n",
    "    dept = random.choice(departments)\n",
    "    large_data.append((name, age, salary, dept))\n",
    "\n",
    "large_df = spark.createDataFrame(large_data, schema)\n",
    "print(f\"ðŸ“Š Dataset grande creato: {large_df.count()} righe\")\n",
    "\n",
    "# Test aggregazione su dataset grande\n",
    "print(\"âš¡ Test performance aggregazione:\")\n",
    "result = large_df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employees\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ").orderBy(\"avg_salary\", ascending=False)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test SQL Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query SQL - Dipendenti 25-45 anni:\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "| department|employee_count|avg_salary|min_salary|max_salary|\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "|      Sales|           971|  95298.68|   40005.0|  149882.0|\n",
      "|    Finance|          1011|  94414.19|   40332.0|  149955.0|\n",
      "|         HR|           958|  94209.39|   40142.0|  149939.0|\n",
      "|Engineering|           910|  93503.43|   40244.0|  149946.0|\n",
      "|  Marketing|           975|  93175.97|   40069.0|  149802.0|\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registra il DataFrame come tabella temporanea\n",
    "large_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Esegui query SQL\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM employees \n",
    "    WHERE age >= 25 AND age <= 45\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"ðŸ” Query SQL - Dipendenti 25-45 anni:\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Cluster Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Informazioni Cluster:\n",
      "Default Parallelism: 14\n",
      "Application ID: app-20251202134751-0000\n",
      "Spark Version: 3.5.7\n",
      "Python Version: 3.12\n"
     ]
    }
   ],
   "source": [
    "# Informazioni sul cluster\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"ðŸ–¥ï¸ Informazioni Cluster:\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Spark Version: {sc.version}\")\n",
    "print(f\"Python Version: {sc.pythonVer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Tabelle temporanee rimosse\n",
      "âœ… Test completato con successo!\n"
     ]
    }
   ],
   "source": [
    "# Pulizia tabelle temporanee\n",
    "spark.catalog.dropTempView(\"employees\")\n",
    "print(\"ðŸ§¹ Tabelle temporanee rimosse\")\n",
    "print(\"âœ… Test completato con successo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
