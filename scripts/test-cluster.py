#!/usr/bin/env python3
"""
Test script per verificare il funzionamento del cluster Spark
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import count, avg
import time

def test_spark_cluster():
    print("ğŸš€ Inizializzazione Spark Session...")
    
    # Crea Spark session
    spark = SparkSession.builder \
        .appName("ClusterTest") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Spark Version: {spark.version}")
    print(f"âœ… Master: {spark.sparkContext.master}")
    print(f"âœ… App Name: {spark.sparkContext.appName}")
    
    # Test parallelismo
    sc = spark.sparkContext
    print(f"âœ… Default Parallelism: {sc.defaultParallelism}")
    
    # Test con dati
    print("\nğŸ“Š Creazione dataset di test...")
    data = [(i, f"user_{i}", i % 5) for i in range(1000)]
    df = spark.createDataFrame(data, ["id", "name", "group"])
    
    print(f"âœ… Dataset creato: {df.count()} righe")
    
    # Test aggregazione
    print("\nâš¡ Test aggregazione (verifica parallelismo)...")
    start_time = time.time()
    
    result = df.groupBy("group").agg(
        count("*").alias("count"),
        avg("id").alias("avg_id")
    ).collect()
    
    end_time = time.time()
    
    print(f"âœ… Aggregazione completata in {end_time - start_time:.2f} secondi")
    print("ğŸ“ˆ Risultati per gruppo:")
    for row in result:
        print(f"   Gruppo {row['group']}: {row['count']} elementi, avg_id = {row['avg_id']:.1f}")
    
    # Test cluster info
    print("\nğŸ–¥ï¸ Informazioni Cluster:")
    print(f"âœ… Master URL: {sc.master}")
    print(f"âœ… Application ID: {sc.applicationId}")
    print(f"âœ… Driver Host: {sc.getConf().get('spark.driver.host', 'unknown')}")
    
    # Test partizioni
    print(f"\nğŸ“¦ Partizioni dataset: {df.rdd.getNumPartitions()}")
    
    # Test caching
    print("\nğŸ’¾ Test caching...")
    df.cache()
    df.count()  # Trigger caching
    cached_count = df.count()  # Dovrebbe essere piÃ¹ veloce
    print(f"âœ… Cache test completato: {cached_count} righe")
    
    spark.stop()
    print("\nğŸ Test completato con successo!")

if __name__ == "__main__":
    test_spark_cluster()